title: Implement tool recording/replay for test fixtures
milestone: Eval Setup
labels:
  - infrastructure
  - evals
depends_on:
  - Set up Promptfoo evaluation framework

body: |
  Record tool calls and responses from real agent runs, replay them in tests.

  ## Why Tool-Level Recording

  - **MCP-compatible**: Works with MCP servers where we don't control HTTP
  - **Tests agent reasoning**: Fixtures are what the agent sees, not internal HTTP
  - **Determinism**: Same tool responses on every test run
  - **Realistic**: Captured from real API interactions

  ## Recording Approach

  Capture tool invocations at the agent SDK level:

  ```typescript
  // Fixture format: request → response pairs
  {
    "tool": "strava_get_activities",
    "request": { "limit": 10, "after": "2024-01-01" },
    "response": { "activities": [...] }
  }
  ```

  ### Recording Mode

  ```typescript
  const tools = wrapToolsForRecording(mcpTools, {
    recordingsDir: 'fixtures/tools',
    mode: process.env.FIXTURE_MODE // 'record' | 'replay'
  });

  // Record: calls real tool, saves response to fixtures/
  // Replay: returns saved response, no tool call
  ```

  ### Matching Strategy

  Tool calls matched by:
  - Tool name (exact)
  - Request parameters (configurable: exact match, subset, or ignore)

  For non-deterministic params (timestamps, pagination), use parameter normalization or sequence-based matching.

  ## Date Handling in Evals

  Weather forecasts and time-sensitive data require careful date handling.

  ### The Problem

  If fixtures were recorded on 2024-06-15 and we replay them with "today" as 2025-01-10:
  - Model knows its training cutoff is later than the fixture date
  - Weather data is historical, not a "forecast" - model might reason differently
  - Model could be confused by temporal inconsistency

  ### Options to Consider

  1. **Mock "today" in system prompt**: Tell the agent "Today is June 15, 2024" and use fixtures from that date. Model reasons as if it's that date.

  2. **Relative date fixtures**: Store fixtures with relative timestamps ("today", "tomorrow") and transform at replay time. More complex but avoids historical confusion.

  3. **Accept the limitation**: Use historical dates, document that evals test reasoning patterns not temporal awareness. May be sufficient for most cases.

  4. **Re-record periodically**: Keep fixtures fresh by re-recording seasonally. High maintenance but most realistic.

  ### Recommendation

  Start with option 1 (mock "today" in system prompt). It's simple and makes the temporal context explicit. If we observe model confusion about dates, revisit.

  ```yaml
  # In eval config
  vars:
    simulated_date: "2024-06-15"
  prompt: |
    Today is {{simulated_date}}.
    Plan a route for: {{query}}
  ```

  ## Tools Requiring Fixtures

  | Tool | Key Calls to Record |
  |------|---------------------|
  | Strava MCP | `get_activities`, `get_segments`, `get_athlete` |
  | GraphHopper | `route`, `elevation` |
  | WeatherKit | `forecast` (multiple locations/times) |
  | Google Maps MCP | `search_places`, `get_place_details` |
  | OSM Overpass | `query` (water fountains, bike infrastructure) |
  | PJAMM | `get_climb`, `search_climbs` |

  ## Privacy: Location Data

  Strava and route fixtures contain GPS coordinates. Before committing:

  - **Strava privacy zones**: Ensure enabled on account (API responses pre-scrubbed)
  - **Home radius scrub**: Truncate points within configured radius of home
  - **ID replacement**: Athlete IDs, names → placeholder values

  Coordinates outside home radius stay real (needed for integration with other tools).

  ## Fixture Structure

  ```
  fixtures/
  └── tools/
      ├── strava/
      │   ├── get_activities.json
      │   └── get_segments.json
      ├── graphhopper/
      │   └── route.json
      ├── weather/
      │   ├── forecast-sunny.json
      │   └── forecast-cold-to-warm.json
      └── places/
          └── search_places.json
  ```

  ## Integration with Promptfoo

  Promptfoo can use recorded fixtures via custom provider:

  ```yaml
  providers:
    - id: file://src/test/recorded-tools-provider.ts
      config:
        fixturesDir: fixtures/tools
  ```

  ## Deliverables

  - [ ] Implement tool wrapper for record/replay
  - [ ] Define fixture JSON schema
  - [ ] Create npm scripts: `fixtures:record`, `fixtures:replay`
  - [ ] Implement home location scrubbing for Strava fixtures
  - [ ] Record initial Strava fixtures
  - [ ] Record weather fixtures for clothing planning evals
  - [ ] Document recording workflow

  ## Success Criteria

  - `npm run evals` works offline
  - Same eval produces identical results on repeated runs
  - No home location exposed in committed fixtures
  - Easy to add fixtures for new scenarios

  ## Note on HTTP Recording (Polly.js)

  For tools implemented as direct HTTP calls (not MCP), Polly.js can still be used.
  But most tools will go through MCP, so tool-level recording is the primary approach.
